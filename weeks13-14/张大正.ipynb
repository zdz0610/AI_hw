{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weeks 13-14\n",
    "\n",
    "## Natural Language Processing\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Feature Engineering\n",
    "We have examined two ways of dealing with categorical (i.e. text based) data: binarizing/dummy variables and numerical scaling. \n",
    "\n",
    "See the following examples for implementation in sklearn to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/categorical.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b97131aca45c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/categorical.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/categorical.csv' does not exist"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/categorical.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c5d84736ba45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarizing\n",
    "Get a list of features you want to binarize, go through each feature and create new features for each level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cd375fe947bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures_to_binarize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Go through each level in this feature (except the last one!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mlevel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;31m# Create new feature for this level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "features_to_binarize = [\"Gender\", \"Marital\"]\n",
    "\n",
    "# Go through each feature\n",
    "for feature in features_to_binarize:\n",
    "    # Go through each level in this feature (except the last one!)\n",
    "    for level in data[feature].unique()[0:-1]:\n",
    "        # Create new feature for this level\n",
    "        data[feature + \"_\" + level] = pd.Series(data[feature] == level, dtype=int)\n",
    "    # Drop original feature\n",
    "    data = data.drop([feature], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c5d84736ba45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric scaling\n",
    "We can also replace text levels with some numeric mapping we create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e0d855c5bc78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m data['Satisfaction'] = data['Satisfaction'].replace(['Very Low', 'Low', 'Neutral', 'High', 'Very High'], \n\u001b[0m\u001b[1;32m      2\u001b[0m                                                     [-2, -1, 0, 1, 2])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data['Satisfaction'] = data['Satisfaction'].replace(['Very Low', 'Low', 'Neutral', 'High', 'Very High'], \n",
    "                                                    [-2, -1, 0, 1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c5d84736ba45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text classification\n",
    "We are going to look at some Amazon reviews and classify them into positive or negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "The file `data/books.csv` contains 2,000 Amazon book reviews. The data set contains two features: the first column (contained in quotes) is the review text. The second column is a binary label indicating if the review is positive or negative.\n",
    "\n",
    "Let's take a quick look at the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: data/books.csv: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 data/books.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data into a pandas data frame. You'll notice two new attributed in `pd.read_csv()` that we've never seen before. The first, `quotechar` is tell us what is being used to \"encapsulate\" the text fields. Since our review text is surrounding by double quotes, we let pandas know. We use a `\\` since the quote is also used to surround the quote. This backslash is known as an escape character. We also let pandas now this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'data/books.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1876d5a529d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/books.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mescapechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\\\\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'data/books.csv' does not exist"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/books.csv\", quotechar=\"\\\"\", escapechar=\"\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-304fa4ce4ebd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text as a set of features\n",
    "Going from text to numeric data is very easy. Let's take a look at how we can do this. We'll start by separating out our X and Y data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-62c51a2e51e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "X_text = data['review_text']\n",
    "Y = data['positive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-51df551f1bcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# look at the first few lines of X_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_text' is not defined"
     ]
    }
   ],
   "source": [
    "# look at the first few lines of X_text\n",
    "X_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-56ea1744c804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# your code here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Y' is not defined"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will turn `X_text` into just `X` -- a numeric representation that we can use in our algorithms or for queries...\n",
    "\n",
    "Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors. \n",
    "\n",
    "The result of the following is a matrix with each row a file and each column a word. The matrix is sparse because most words only appear a few times. The values are 1 if a word appears in a document and 1 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a417ba8012fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Let the vectorizer learn what tokens exist in the text data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbinary_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Turn these tokens into a numeric matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_text' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a vectorizer that will track text as binary features\n",
    "binary_vectorizer = CountVectorizer(binary=True)\n",
    "\n",
    "# Let the vectorizer learn what tokens exist in the text data\n",
    "binary_vectorizer.fit(X_text)\n",
    "\n",
    "# Turn these tokens into a numeric matrix\n",
    "X = binary_vectorizer.transform(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-61af2c1cc8cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dimensions of X:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Dimensions of X:\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2000 documents (each row) and 22,743 words/tokens.\n",
    "\n",
    "Can look at some of the words by querying the binary vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hygi',\n",
       " 'hygience',\n",
       " 'hygiene',\n",
       " 'hyojin',\n",
       " 'hype',\n",
       " 'hyped',\n",
       " 'hyper',\n",
       " 'hyperbole',\n",
       " 'hyperpat',\n",
       " 'hyperpussiance',\n",
       " 'hyperspace',\n",
       " 'hypnosis',\n",
       " 'hypnotic',\n",
       " 'hypnotism',\n",
       " 'hypnotizing',\n",
       " 'hypocrisy',\n",
       " 'hypocrite',\n",
       " 'hypocritical',\n",
       " 'hypocritically',\n",
       " 'hypoglycemia']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of the 20 features (words) in column 10,000\n",
    "features = binary_vectorizer.get_feature_names()\n",
    "features[10000:10020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spend some time to look at the binary vectoriser.\n",
    "\n",
    "Examine the structure of X. Look at some the rows and columns values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11646c2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see the density of 0s and 1s in X\n",
    "import scipy.sparse as sps\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.spy(X.toarray())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the sparse matrix above. Notice how some columns are quite dark (i.e. the words appear in almost every file). \n",
    "\n",
    "What are the 5 most common words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zzzzzzzzzzzz', 'zzzz', 'zylpha', 'zukav', 'zucker']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "word_dict=binary_vectorizer.vocabulary_\n",
    "word_sorted=dict(sorted(word_dict.items(), key=lambda d: d[1],reverse=True))\n",
    "list(word_sorted.keys())[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes the sparse matrix X, and gets the feature list from the vectoriser, and a document index (1 - 2000) and returns a list of the words in the file that corresponds to the index (the list should be obtained from the sparse matrix / bag of words representation NOT from the original data file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13',\n",
       " '33',\n",
       " 'about',\n",
       " 'after',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'avid',\n",
       " 'back',\n",
       " 'better',\n",
       " 'book',\n",
       " 'boy',\n",
       " 'copy',\n",
       " 'could',\n",
       " 'don',\n",
       " 'entire',\n",
       " 'fire',\n",
       " 'for',\n",
       " 'friend',\n",
       " 'from',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'had',\n",
       " 'half',\n",
       " 'have',\n",
       " 'headache',\n",
       " 'horrible',\n",
       " 'if',\n",
       " 'in',\n",
       " 'it',\n",
       " 'less',\n",
       " 'life',\n",
       " 'lit',\n",
       " 'lower',\n",
       " 'man',\n",
       " 'mom',\n",
       " 'money',\n",
       " 'my',\n",
       " 'of',\n",
       " 'old',\n",
       " 'on',\n",
       " 'one',\n",
       " 'part',\n",
       " 'picked',\n",
       " 'possible',\n",
       " 'purposes',\n",
       " 'rate',\n",
       " 'read',\n",
       " 'reader',\n",
       " 'reading',\n",
       " 'relationship',\n",
       " 'so',\n",
       " 'spent',\n",
       " 'star',\n",
       " 'suffering',\n",
       " 'than',\n",
       " 'the',\n",
       " 'then',\n",
       " 'this',\n",
       " 'time',\n",
       " 'to',\n",
       " 'up',\n",
       " 'use',\n",
       " 'was',\n",
       " 'waste',\n",
       " 'wasted',\n",
       " 'wish',\n",
       " 'with',\n",
       " 'world',\n",
       " 'would',\n",
       " 'year',\n",
       " 'your']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# complete the function \n",
    "# returns vector of words / features\n",
    "def getWords(bag_of_words, file_index_row, features_list):\n",
    "    word_marks=bag_of_words.toarray()[file_index_row-1]\n",
    "    return [features_list[i] for i in range(len(word_marks)) if word_marks[i]==1]\n",
    "getWords(X, 1, features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "We have a 22743 features, let's use them in some different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy of our classifier is 0.768\n"
     ]
    }
   ],
   "source": [
    "# Create a model\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Use this model and our data to get 5-fold cross validation accuracy\n",
    "acc = cross_val_score(logistic_regression, X, Y, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "# Print out the average accuracy rounded to three decimal points\n",
    "print (\"Mean accuracy of our classifier is \" + str(round(np.mean(acc), 3)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Use the above classifier to classify a new example (new review below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_review = \"\"\"\n",
    "really bad book!\n",
    "\"\"\"\n",
    "\n",
    "# your code here ...\n",
    "logistic_regression.fit(X,Y)\n",
    "x_test=pd.DataFrame({\"x_test\":[new_review]})\n",
    "x_test=binary_vectorizer.transform(x_test)\n",
    "logistic_regression.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using full counts instead of a binary representation (i.e. each time a word appears use the raw count value). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for our classifier is 0.786\n"
     ]
    }
   ],
   "source": [
    "# Create a vectorizer that will track text as binary features\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Let the vectorizer learn what tokens exist in the text data\n",
    "count_vectorizer.fit(X_text)\n",
    "\n",
    "# Turn these tokens into a numeric matrix\n",
    "X = count_vectorizer.transform(X_text)\n",
    "\n",
    "# Create a model\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Use this model and our data to get 5-fold cross validation accuracy\n",
    "acc = cross_val_score(logistic_regression, X, Y, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "# Print out the average AUC rounded to three decimal points\n",
    "print( \"Accuracy for our classifier is \" + str(round(np.mean(acc), 3)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try using TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for our classifier is 0.784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Create a vectorizer that will track text as binary features\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Let the vectorizer learn what tokens exist in the text data\n",
    "tfidf_vectorizer.fit(X_text)\n",
    "\n",
    "# Turn these tokens into a numeric matrix\n",
    "X = tfidf_vectorizer.transform(X_text)\n",
    "\n",
    "# Create a model\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Use this model and our data to get 5-fold cross validation AUCs\n",
    "acc = cross_val_score(logistic_regression, X, Y, scoring=\"accuracy\", cv=5)\n",
    "\n",
    "# Print out the average AUC rounded to three decimal points\n",
    "print( \"Accuracy for our classifier is \" + str(round(np.mean(acc), 3)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the tfidf classifier to classify some online book reviews from here: https://www.amazon.com/\n",
    "\n",
    "Hint: You can copy and paste a review from the online site into a multiline string literal with 3 quotes: \n",
    "```\n",
    "\"\"\"\n",
    "I read Princess Elizabeth's Spy last week; funny to think that I picked this one up next, with the theme so similar. It isn't of any deep literary value but it is a really good read when you, as I am, are feeling under the weather and fuzzy headed. Although I had a suspicion from the second chapter (and was ultimately proved right) as to what was going to happen, Bowen's writing is crisp and flows well so I kept reading every page wondering how she was going to connect all the dots.\n",
    "\n",
    "The story revolves around a stately home and three (well four but one is a ditz) aristocratic daughters who each in their own way become involved in World War 2 spy work. My mind compared them to the sisters in Downton Abbey but the book is original enough not to be any more of a copycat than others of that ilk. Still, anyone who enjoyed Downton Abbey will probably enjoy this one.\n",
    "\n",
    "There is danger and some allusions to sex but in the main the book relies more on suspense. If you are looking for a serious World War 2 spy novel, this is not it but I enjoyed it.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "new_review = \"\"\"\n",
    "I read Princess Elizabeth's Spy last week; funny to think that I picked this one up next, with the theme so similar. It isn't of any deep literary value but it is a really good read when you, as I am, are feeling under the weather and fuzzy headed. Although I had a suspicion from the second chapter (and was ultimately proved right) as to what was going to happen, Bowen's writing is crisp and flows well so I kept reading every page wondering how she was going to connect all the dots.\n",
    "The story revolves around a stately home and three (well four but one is a ditz) aristocratic daughters who each in their own way become involved in World War 2 spy work. My mind compared them to the sisters in Downton Abbey but the book is original enough not to be any more of a copycat than others of that ilk. Still, anyone who enjoyed Downton Abbey will probably enjoy this one.\n",
    "There is danger and some allusions to sex but in the main the book relies more on suspense. If you are looking for a serious World War 2 spy novel, this is not it but I enjoyed it.\n",
    "\"\"\"\n",
    "\n",
    "# your code here ...\n",
    "logistic_regression.fit(X,Y)\n",
    "x_test=pd.DataFrame({\"x_test\":[new_review]})\n",
    "x_test=tfidf_vectorizer.transform(x_test)\n",
    "logistic_regression.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the implementation\n",
    "#### Features\n",
    "Tfidf is looking pretty good! How about adding n-grams? Stop words? Lowercase transforming?\n",
    "\n",
    "We saw that the most common words include \"the\" and others above - start by making these stop words.\n",
    "\n",
    "N-grams are conjunctions of words (e.g. a 2-gram adds all sequences of 2 words)\n",
    "\n",
    "\n",
    "Look at the docs: `CountVectorizer()` and `TfidfVectorizer()` can be modified to handle all of these things. Work in groups and try a few different combinations of these settings for anything you want: binary counts, numeric counts, tf-idf counts. Here is how you would use these settings:\n",
    "\n",
    "- \"`ngram_range=(1,2)`\": would include unigrams and bigrams (ie including combinations of words in sequence)\n",
    "- \"`stop_words=\"english\"`\": would use a standard set of English stop words\n",
    "- \"`lowercase=False`\": would turn off lowercase transformation (it is actually on by default)!\n",
    "\n",
    "You can use some of these like this:\n",
    "\n",
    "`tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), lowercase=False)`\n",
    "\n",
    "#### Models\n",
    "Next swap out the line creating a logistic regression with one making a naive Bayes or support vector machines (SVM). SVM have been shown to be very effective in text classification. Naive Bayes has been used a lot also.\n",
    "\n",
    "For example see: http://www.cs.cornell.edu/home/llee/papers/sentiment.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for our classifier is 0.782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC \n",
    "clf=LinearSVC(C=0.2)\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True,stop_words=\"english\")\n",
    "tfidf_vectorizer.fit(X_text)\n",
    "X = tfidf_vectorizer.transform(X_text)\n",
    "acc = cross_val_score(clf, X.toarray(), Y, scoring=\"accuracy\", cv=5)\n",
    "print( \"Accuracy for our classifier is \" + str(round(np.mean(acc), 3)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for our classifier is 0.782\n",
      "Accuracy for our classifier is 0.78\n",
      "Accuracy for our classifier is 0.78\n",
      "Accuracy for our classifier is 0.78\n",
      "Accuracy for our classifier is 0.781\n",
      "Accuracy for our classifier is 0.781\n",
      "Accuracy for our classifier is 0.781\n",
      "Accuracy for our classifier is 0.781\n",
      "Accuracy for our classifier is 0.782\n",
      "Accuracy for our classifier is 0.781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from numpy import arange\n",
    "for c in arange(0.2,0.3,0.01):\n",
    "    clf=LinearSVC(C=c)\n",
    "    tfidf_vectorizer = TfidfVectorizer(lowercase=True,stop_words=\"english\")\n",
    "    tfidf_vectorizer.fit(X_text)\n",
    "    X = tfidf_vectorizer.transform(X_text)\n",
    "    acc = cross_val_score(clf, X.toarray(), Y, scoring=\"accuracy\", cv=5)\n",
    "    print( \"Accuracy for our classifier is \" + str(round(np.mean(acc), 3)) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
